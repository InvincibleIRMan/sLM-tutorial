{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f8835d-cb21-4118-b642-fa56d390a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, threading\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextIteratorStreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f9caf-52dc-4196-a641-61715c525860",
   "metadata": {},
   "source": [
    "### ðŸ“š Dataset Overview: `qiaojin/PubMedQA`\n",
    "\n",
    "The **PubMedQA** dataset is a biomedical question-answering benchmark derived from **PubMed** abstracts.  \n",
    "It was introduced by *Qiao Jin et al.* (EMNLP 2019) to evaluate language models on factual reasoning over scientific literature.\n",
    "\n",
    "This Hugging Face version â€” **`qiaojin/PubMedQA`** â€” provides three configurations:\n",
    "\n",
    "- **`pqa_labeled`** â€“ 1,000 expert-annotated *yes/maybe/no* questions with their corresponding PubMed abstracts and detailed long answers.  \n",
    "  *Recommended for supervised fine-tuning.*\n",
    "- **`pqa_artificial`** â€“ automatically generated Q&A pairs created from PubMed titles and abstracts.  \n",
    "- **`pqa_unlabeled`** â€“ questions collected from PubMed titles without gold answers.\n",
    "\n",
    "**Summary:**  \n",
    "\n",
    "| Config name        | Description                                                              | Use case                                   |\n",
    "| ------------------ | ------------------------------------------------------------------------ | ------------------------------------------ |\n",
    "| `\"pqa_labeled\"`    | 1,000 expert-annotated questions with yes/maybe/no answers and abstracts | **Recommended** for supervised fine-tuning |\n",
    "| `\"pqa_artificial\"` | Automatically generated Qâ€“A pairs                                        | Good for pre-training / augmentation       |\n",
    "| `\"pqa_unlabeled\"`  | Questions without human answers                                          | For semi-supervised or retrieval testing   |\n",
    "\n",
    "\n",
    "Each entry in the labeled set contains:\n",
    "\n",
    "| Field | Description |\n",
    "|--------|--------------|\n",
    "| `pubid` | PubMed ID of the source article |\n",
    "| `question` | A research-style yes/no/maybe question |\n",
    "| `context` | The relevant abstract text |\n",
    "| `long_answer` | A natural-language justification of the answer |\n",
    "| `final_decision` | The categorical label (`yes`, `no`, or `maybe`) |\n",
    "\n",
    "> The dataset supports tasks such as biomedical question answering, literature-based reasoning, and fine-tuning domain-specific LLMs for evidence-grounded responses.\n",
    "\n",
    "**Citation:**  \n",
    "> Jin Q., Dhingra B., Liu Z., Cohen W.W., & Lu X. (2019).  \n",
    "> *PubMedQA: A Dataset for Biomedical Research Question Answering.*  \n",
    "> Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed432907-18de-4158-9838-0ebbfc10abc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anchor': 'Does a history of unintended pregnancy lessen the likelihood of desire for sterilization reversal?', 'positive': 'Unintended pregnancy has been significantly associated with subsequent female sterilization. Whether women who are sterilized after experiencing an unintended pregnancy are less likely to express desire for sterilization reversal is unknown.', 'negative': 'Changes in serum hormone levels induced by combined contraceptives.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "ds = load_dataset(\"sentence-transformers/pubmedqa\", name=\"triplet-all\", split=\"train\")\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d534efa-32c1-4017-9e26-850808ef8dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"qiaojin/PubMedQA\", name=\"pqa_labeled\", split=\"train\")\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1844f6be-e548-4f7e-87bd-3a11ae92cd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'You are a biomedical research assistant. Be concise, cite PMIDs when possible.', 'role': 'system'}, {'content': \"Question: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\\n\\nAbstract (PMID:21645374): {'contexts': ['Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cells at the center of these areoles and progresses outwards, stopping approximately five cells from the vasculature. The role of mitochondria during PCD has been recognized in animals; however, it has been less studied during PCD in plants.', 'The following paper elucidates the role of mitochondrial dynamics during developmentally regulated PCD in vivo in A. madagascariensis. A single areole within a window stage leaf (PCD is occurring) was divided into three areas based on the progression of PCD; cells that will not undergo PCD (NPCD), cells in early stages of PCD (EPCD), and cells in late stages of PCD (LPCD). Window stage leaves were stained with the mitochondrial dye MitoTracker Red CMXRos and examined. Mitochondrial dynamics were delineated into four categories (M1-M4) based on characteristics including distribution, motility, and membrane potential (Î”Î¨m). A TUNEL assay showed fragmented nDNA in a gradient over these mitochondrial stages. Chloroplasts and transvacuolar strands were also examined using live cell imaging. The possible importance of mitochondrial permeability transition pore (PTP) formation during PCD was indirectly examined via in vivo cyclosporine A (CsA) treatment. This treatment resulted in lace plant leaves with a significantly lower number of perforations compared to controls, and that displayed mitochondrial dynamics similar to that of non-PCD cells.'], 'labels': ['BACKGROUND', 'RESULTS'], 'meshes': ['Alismataceae', 'Apoptosis', 'Cell Differentiation', 'Mitochondria', 'Plant Leaves'], 'reasoning_required_pred': ['y', 'e', 's'], 'reasoning_free_pred': ['y', 'e', 's']}\", 'role': 'user'}, {'content': 'Results depicted mitochondrial dynamics in vivo as PCD progresses within the lace plant, and highlight the correlation of this organelle with other organelles during developmental PCD. To the best of our knowledge, this is the first report of mitochondria and chloroplasts moving on transvacuolar strands to form a ring structure surrounding the nucleus during developmental PCD. Also, for the first time, we have shown the feasibility for the use of CsA in a whole plant system. Overall, our findings implicate the mitochondria as playing a critical and early role in developmentally regulated PCD in the lace plant. [PMID:21645374]', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "def to_messages(ex):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a biomedical research assistant. Be concise, cite PMIDs when possible.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {ex['question']}\\n\\nAbstract (PMID:{ex['pubid']}): {ex['context']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"{ex['long_answer']} [PMID:{ex['pubid']}]\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "chat_ds = ds.map(to_messages, remove_columns=ds.column_names)\n",
    "print(chat_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc3649c5-72e6-41fa-8b12-8d9fac8be586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe47c0e8350>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, os, random\n",
    "from datasets import load_dataset, disable_progress_bar\n",
    "disable_progress_bar()  # avoids widget issues in VS Code notebooks\n",
    "\n",
    "# Pick a model size: start small, then scale up\n",
    "MODEL_NAME = \"unsloth/gpt-oss-7b-instruct-bnb-4bit\"  # fast prototype\n",
    "# For larger run later: \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\"\n",
    "\n",
    "OUT_DIR = \"gptoss_pubmedqa_sft\"\n",
    "SEED = 3407\n",
    "MAX_SEQ_LEN = 2048\n",
    "MAX_STEPS = 800          # small pilot; increase when happy\n",
    "BATCH_PER_DEVICE = 1\n",
    "GRAD_ACCUM = 8           # effective batch = BATCH_PER_DEVICE * GRAD_ACCUM * n_gpus\n",
    "LR = 2e-4\n",
    "WARMUP_STEPS = 50\n",
    "PACKING = True\n",
    "EVAL_SAMPLES = 256       # small, fast evaluation subset\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee1b1c1-7a47-466b-a393-bf5822f460a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pubid': 11458136, 'final_decision': 'maybe', 'messages': [{'content': 'You are a biomedical research assistant. Be accurate, concise, and always cite PMIDs in brackets.', 'role': 'system'}, {'content': \"Question: Does managed care enable more low income persons to identify a usual source of care?\\n\\nAbstract (PMID:11458136): {'contexts': ['By requiring or encouraging enrollees to obtain a usual source of care, managed care programs hope to improve access to care without incurring higher costs.', '(1) To examine the effects of managed care on the likelihood of low-income persons having a usual source of care and a usual physician, and; (2) To examine the association between usual source of care and access.', 'Cross-sectional survey of households conducted during 1996 and 1997.', 'A nationally representative sample of 14,271 low-income persons.', 'Usual source of care, usual physician, managed care enrollment, managed care penetration.', 'High managed care penetration in the community is associated with a lower likelihood of having a usual source of care for uninsured persons (54.8% vs. 62.2% in low penetration areas) as well as a lower likelihood of having a usual physician (60% vs. 72.8%). Managed care has only marginal effects on the likelihood of having a usual source of care for privately insured and Medicaid beneficiaries. Having a usual physician substantially reduces unmet medical needs for the insured but less so for the uninsured.'], 'labels': ['BACKGROUND', 'OBJECTIVES', 'RESEARCH DESIGN', 'SUBJECTS', 'MEASURES', 'RESULTS'], 'meshes': ['Adult', 'Continuity of Patient Care', 'Cross-Sectional Studies', 'Female', 'Health Services Accessibility', 'Humans', 'Insurance Coverage', 'Male', 'Managed Care Programs', 'Medically Uninsured', 'Multivariate Analysis', 'Poverty', 'United States'], 'reasoning_required_pred': ['n', 'o'], 'reasoning_free_pred': ['m', 'a', 'y', 'b', 'e']}\\n\\nTask: Answer concisely and cite the PMID in brackets.\", 'role': 'user'}, {'content': 'Having a usual physician can be an effective tool in improving access to care for low-income populations, although it is most effective when combined with insurance coverage. However, the effectiveness of managed care in linking more low-income persons to a medical home is uncertain, and may have unintended consequences for uninsured persons. [PMID:11458136]', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load labeled split\n",
    "raw = load_dataset(\"qiaojin/PubMedQA\", name=\"pqa_labeled\")\n",
    "# Typical splits may be missing; if only \"train\" exists, create your own split:\n",
    "if \"train\" in raw and \"validation\" not in raw:\n",
    "    raw = raw[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
    "    train_ds, eval_ds = raw[\"train\"], raw[\"test\"]\n",
    "else:\n",
    "    train_ds = raw[\"train\"]\n",
    "    eval_ds  = raw.get(\"validation\", raw[\"train\"].select(range(min(EVAL_SAMPLES, len(raw[\"train\"])))))\n",
    "\n",
    "def to_messages(ex):\n",
    "    # Build instruction style with citations\n",
    "    q   = ex.get(\"question\", \"\")\n",
    "    ctx = ex.get(\"context\", \"\") or \"\"\n",
    "    pmid = ex.get(\"pubid\", \"\")\n",
    "    long_ans = (ex.get(\"long_answer\", \"\") or \"\").strip()\n",
    "    final = (ex.get(\"final_decision\",\"\") or \"\").strip()  # yes/no/maybe\n",
    "\n",
    "    user = f\"Question: {q}\\n\\nAbstract (PMID:{pmid}): {ctx}\\n\\n\" \\\n",
    "           f\"Task: Answer concisely and cite the PMID in brackets.\"\n",
    "    assistant = f\"{long_ans} [PMID:{pmid}]\".strip()\n",
    "\n",
    "    return {\"messages\":[\n",
    "        {\"role\":\"system\",    \"content\":\"You are a biomedical research assistant. Be accurate, concise, and always cite PMIDs in brackets.\"},\n",
    "        {\"role\":\"user\",      \"content\": user},\n",
    "        {\"role\":\"assistant\", \"content\": assistant}\n",
    "    ],\n",
    "    \"final_decision\": final, \"pubid\": pmid}\n",
    "\n",
    "train_chat = train_ds.map(to_messages, remove_columns=train_ds.column_names)\n",
    "eval_chat  = eval_ds.map(to_messages,  remove_columns=eval_ds.column_names)\n",
    "print(train_chat[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd71b2c8-37d1-4550-837a-aead9257ae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.11: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.564 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gpt_oss won't work! Using float32.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540ed2e8a8924345a340c4214a75d9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GptOssForCausalLM(\n",
       "  (model): GptOssModel(\n",
       "    (embed_tokens): Embedding(201088, 2880, padding_idx=199999)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GptOssDecoderLayer(\n",
       "        (self_attn): GptOssAttention(\n",
       "          (q_proj): Linear4bit(in_features=2880, out_features=4096, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=2880, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=2880, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=2880, bias=True)\n",
       "        )\n",
       "        (mlp): GptOssMLP(\n",
       "          (router): GptOssTopKRouter(\n",
       "            (linear): Linear(in_features=2880, out_features=32, bias=True)\n",
       "          )\n",
       "          (experts): GptOssExperts(\n",
       "            (gate_up_projs): ModuleList(\n",
       "              (0-31): 32 x Linear4bit(in_features=2880, out_features=5760, bias=True)\n",
       "            )\n",
       "            (down_projs): ModuleList(\n",
       "              (0-31): 32 x Linear4bit(in_features=2880, out_features=2880, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "        (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "    (rotary_emb): GptOssRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2880, out_features=201088, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "MODEL_NAME = \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\"  # confirmed on HF\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name      = MODEL_NAME,\n",
    "    dtype           = None,\n",
    "    max_seq_length  = MAX_SEQ_LEN,\n",
    "    load_in_4bit    = True,\n",
    "    full_finetuning = False,\n",
    ")\n",
    "\n",
    "# Prepare for inference forward passes (saves mem), then attach LoRA\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6980ad66-ca42-4ba8-866a-7121305c1289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",   # or a list like [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f096beb1-5bc8-4922-a144-05888c7a9adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "device = next(model.parameters()).device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506fbb50-f6d8-4a01-992a-423d55ee029a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "def render_chat(messages, tokenizer: PreTrainedTokenizerBase, max_len: int):\n",
    "    # Render using model's chat template; include assistant text (SFT)\n",
    "    rendered = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=False,   # training sees assistant outputs\n",
    "        tokenize=False,\n",
    "    )\n",
    "    ids = tokenizer(\n",
    "        rendered, truncation=True, max_length=max_len, return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\"input_ids\": ids[\"input_ids\"][0], \"attention_mask\": ids[\"attention_mask\"][0]}\n",
    "\n",
    "def build_supervised(ex, tokenizer, max_len):\n",
    "    out = render_chat(ex[\"messages\"], tokenizer, max_len)\n",
    "    out[\"labels\"] = out[\"input_ids\"].clone()\n",
    "    return out\n",
    "\n",
    "proc = partial(build_supervised, tokenizer=tokenizer, max_len=MAX_SEQ_LEN)\n",
    "train_tok = train_chat.map(proc, remove_columns=train_chat.column_names)\n",
    "eval_tok  = eval_chat.map(proc,  remove_columns=eval_chat.column_names)\n",
    "\n",
    "\n",
    "train_dataset = train_tok\n",
    "\n",
    "len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30894b3f-383e-477a-9dd9-80e39460164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "\n",
    "has_cuda = torch.cuda.is_available()\n",
    "bf16_ok  = has_cuda and torch.cuda.is_bf16_supported()   # Ampere+ with proper CUDA\n",
    "# fp16 is widely supported on NVIDIA GPUs even if bf16 isn't\n",
    "fp16_ok  = has_cuda and not bf16_ok\n",
    "\n",
    "# If you sometimes run on CPU, also switch the optimizer (bitsandbytes needs CUDA)\n",
    "optim_name = \"paged_adamw_8bit\" if has_cuda else \"adamw_torch\"\n",
    "\n",
    "from trl import SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_PER_DEVICE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    max_steps=MAX_STEPS,           # or use num_train_epochs\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    bf16=bf16_ok,                  # <-- only True on Ampere+\n",
    "    fp16=fp16_ok,                  # <-- True on pre-Ampere NVIDIA GPUs\n",
    "    gradient_checkpointing=True,\n",
    "    optim=optim_name,              # bitsandbytes on GPU, torch AdamW on CPU\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=\"none\",\n",
    "    packing=PACKING,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "913f6b51-a629-4142-8195-c3dbfac166cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this once, BEFORE instantiating SFTTrainer\n",
    "import unsloth_zoo.logging_utils as _lu\n",
    "_lu.PatchRLStatistics = lambda *args, **kwargs: None  # disable Unsloth's RL stats patch temporarily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3aec3cb-6b09-4b64-a4c5-79950fc70fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Switching to float32 training since model cannot work with float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/codes/test-unsloth/tutorials/sLM-tutorial/unsloth_compiled_cache/UnslothSFTTrainer.py:674: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/codes/test-unsloth/tutorials/sLM-tutorial/unsloth_compiled_cache/UnslothSFTTrainer.py:712: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/codes/test-unsloth/tutorials/sLM-tutorial/unsloth_compiled_cache/UnslothSFTTrainer.py:807: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `UnslothSFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "/home/ubuntu/codes/test-unsloth/tutorials/sLM-tutorial/unsloth_compiled_cache/UnslothSFTTrainer.py:826: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def to_text(ex):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            ex[\"messages\"],\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    }\n",
    "\n",
    "train_text = train_chat.map(to_text, remove_columns=train_chat.column_names)\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_text,   # now has a \"text\" field\n",
    "    dataset_text_field = \"text\",  # <-- tell TRL which field to use\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"paged_adamw_8bit\" if torch.cuda.is_available() else \"adamw_torch\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "        packing = True,\n",
    "        bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "        fp16 = torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
    "        gradient_checkpointing = True,\n",
    "        save_steps = 200,\n",
    "        save_total_limit = 2,\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726fe2b-eab1-418c-ba41-1225faeebaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998, 'pad_token_id': 200017}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 302 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 7,962,624 of 20,922,719,808 (0.04% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/30 08:35 < 13:15, 0.02 it/s, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.872700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.868500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.584700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.086400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.269500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.162600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.845800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.733000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197a59e-0077-4a8f-961c-73b873db2a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998, 'pad_token_id': 200017}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 281 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 7,962,624 of 20,922,719,808 (0.04% trained)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89579caa-d17f-4046-a30a-4339564c181f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
