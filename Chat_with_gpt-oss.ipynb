{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ad723e",
   "metadata": {},
   "source": [
    "# üí¨ Chat with Unsloth GPT-OSS (Streaming in a Notebook)\n",
    "\n",
    "This notebook demonstrates a **minimal chat loop** using the **Unsloth** `FastLanguageModel` with **streaming generation** via Hugging Face‚Äôs `TextIteratorStreamer`.\n",
    "\n",
    "It supports both **full-precision** and **4-bit quantized** models, and shows how to set the `reasoning_effort` parameter in the chat template.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Overview\n",
    "\n",
    "**You‚Äôll learn how to:**\n",
    "- Load and prepare an Unsloth model & tokenizer  \n",
    "- Stream responses token-by-token  \n",
    "- Control sampling (temperature, top-p, etc.)  \n",
    "- Run an interactive REPL chat loop  \n",
    "- Adjust reasoning effort (`\"low\"`, `\"medium\"`, `\"high\"`)\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Requirements\n",
    "\n",
    "**Install dependencies:\n",
    "\n",
    "```bash\n",
    "pip install unsloth transformers accelerate\n",
    "# For 4-bit models:\n",
    "pip install bitsandbytes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf530f-c9dd-456d-b4e9-acb51ab352b4",
   "metadata": {},
   "source": [
    "## ‚úÖ Recommended Environment Note\n",
    "\n",
    "**Note:**  \n",
    "> For interactive sessions that use the Python `input()` function,  \n",
    "> please run this notebook in a **web browser (Jupyter Notebook or JupyterLab)** `using jupyter lab --no-browser --port 8888` command.  \n",
    "> The **VS Code Jupyter extension** currently does not fully support interactive `input()` prompts,  \n",
    "> which may cause the session to hang or fail to capture user input correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c61dc",
   "metadata": {},
   "source": [
    "## üß† Model Options\n",
    "\n",
    "**You can use either full precision or 4-bit quantized models:\n",
    "\n",
    "| Type         | Model Name                                                                      |\n",
    "| ------------ | ------------------------------------------------------------------------------- |\n",
    "| Full / MXFP4 | `unsloth/gpt-oss-20b`, `unsloth/gpt-oss-120b`                                   |\n",
    "| 4-bit (bnb)  | `unsloth/gpt-oss-20b-unsloth-bnb-4bit`, `unsloth/gpt-oss-120b-unsloth-bnb-4bit` |\n",
    "\n",
    "To enable 4-bit quantization, set:\n",
    "\n",
    "load_in_4bit = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b28869a",
   "metadata": {},
   "source": [
    "\n",
    "## ‚öôÔ∏è Key Parameters\n",
    "\n",
    "| Parameter                | Description                       |\n",
    "| ------------------------ | --------------------------------- |\n",
    "| `max_seq_length=4096`    | Context length for the model      |\n",
    "| `temperature=0.7`        | Sampling temperature (creativity) |\n",
    "| `top_p=0.9`              | Nucleus sampling probability      |\n",
    "| `reasoning_effort=\"low\"` | Adjusts reasoning complexity      |\n",
    "| `max_new_tokens=256`     | Number of tokens to generate      |\n",
    "| `do_sample=True`         | Enables stochastic generation     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c91776",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a92610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.11: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.564 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gpt_oss won't work! Using float32.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7238e5999de461a9580f3aa76e15b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, torch, threading\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
    "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
    "    \"unsloth/gpt-oss-120b\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 4096, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "device = next(model.parameters()).device\n",
    "tokenizer.pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd4197d",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Initialize conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad34698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise and friendly.\"},\n",
    "]\n",
    "\n",
    "# Example REPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7843fa4",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Chat function with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03557f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_once(user_text: str) -> str:\n",
    "    messages.append({\"role\":\"user\",\"content\":user_text})\n",
    "    inp = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", reasoning_effort=\"low\").to(device)\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    gen_kwargs = dict(\n",
    "        input_ids=inp,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True, top_p=0.9, temperature=0.7,\n",
    "        eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True, streamer=streamer,\n",
    "    )\n",
    "    t = threading.Thread(target=model.generate, kwargs=gen_kwargs); t.start()\n",
    "    chunks=[]; print(\"Assistant: \", end=\"\", flush=True)\n",
    "    for tok in streamer: print(tok, end=\"\", flush=True); chunks.append(tok)\n",
    "    print()\n",
    "    reply = \"\".join(chunks).strip()\n",
    "    messages.append({\"role\":\"assistant\",\"content\":reply})\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5cc83f",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Interactive loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88950614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Hi there, how are you doing? I am not feeling well today \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: analysisNeed to respond empathetically.assistantfinalHi! I‚Äôm just a bunch of code, so I don‚Äôt have feelings, but I‚Äôm here to help. I‚Äôm sorry to hear you‚Äôre not feeling well. Is there anything I can do to help you feel better today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Well, I have a headache and some fever.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: analysisNeed to give advice, not medical diagnosis.assistantcommentary to=functions.run code{\"name\":\"browser_search\",\"arguments\":\"{\\\"query\\\":\\\"headache fever advice\\\"}\"}commentarycommentary to=functions.runcommentary{\"name\":\"browser_search\",\"arguments\":\"{\\\"query\\\":\\\"headache fever advice\\\"}\"}commentaryanalysisWe got results.assistantfinalI‚Äôm sorry to hear that‚Äîthose symptoms can be pretty uncomfortable. While I‚Äôm not a medical professional, here are a few general suggestions that might help you feel a bit better:\n",
      "\n",
      "| Symptom | Quick Self‚ÄëCare Tips |\n",
      "|---------|----------------------|\n",
      "| **Headache** | ‚Ä¢ Rest in a quiet, dimly lit room.<br>‚Ä¢ Stay hydrated ‚Äì sip water, herbal tea, or a clear broth.<br>‚Ä¢ Try a cool compress on your forehead or neck. |\n",
      "| **Fever** | ‚Ä¢ Take a lukewarm shower or bath; avoid hot water which can raise your core temperature.<br>‚Ä¢ Wear loose, breathable clothing.<br>‚Ä¢ Keep your room at a comfortable temperature (around 20‚Äì22‚ÄØ¬∞C). |\n",
      "\n",
      "### What to do next\n",
      "1. **Monitor\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Great but what if I still do not feel well?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: analysisNeed to advise to see doctor.assistantfinalI‚Äôm sorry you‚Äôre still feeling unwell. If the headache and fever persist or worsen, it‚Äôs best to seek professional help. Here‚Äôs what you can do next:\n",
      "\n",
      "1. **Contact a healthcare provider** ‚Äì Call your doctor, nurse line, or go to an urgent‚Äëcare clinic if symptoms are severe (e.g., high fever >39‚ÄØ¬∞C, persistent headache, difficulty breathing, confusion, or any new symptoms).\n",
      "\n",
      "2. **Keep a symptom log** ‚Äì Note the temperature, pain level, any other symptoms (nausea, rash, etc.). This information will be useful for the clinician.\n",
      "\n",
      "3. **Follow any medical advice** ‚Äì If you‚Äôre prescribed medication, take it as directed. If you‚Äôre told to rest, stay in a quiet, comfortable environment.\n",
      "\n",
      "4. **Monitor for red‚Äëflag signs** ‚Äì If you develop:\n",
      "   - Sudden severe headache (‚Äúthunderclap‚Äù headache)\n",
      "   - Persistent vomiting or diarrhea\n",
      "   - Difficulty breathing or chest pain\n",
      "   - Severe or worsening fever\n",
      "   ‚Äì seek emergency care immediately.\n",
      "\n",
      "5. **Take care of yourself** ‚Äì Drink fluids, rest, and avoid strenuous activity until you feel better.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Glad I could help. üëã\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    s = input(\"\\nYou: \").strip()\n",
    "    if s.lower() in {\"bye\",\"quit\",\"exit\",\"thanks\",\"thank you, the problem has been resolved\"}:\n",
    "        print(\"Assistant: Glad I could help. üëã\"); break\n",
    "    chat_once(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbad048",
   "metadata": {},
   "source": [
    "## üßÆ Example Modifications\n",
    "\n",
    "- Shorter / longer outputs: adjust max_new_tokens\n",
    "\n",
    "- Deterministic: set do_sample=False\n",
    "\n",
    "- Change assistant tone: modify the system message\n",
    "\n",
    "- Reduce GPU memory usage: switch to 4-bit with load_in_4bit=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b982f1bb",
   "metadata": {},
   "source": [
    "## üöÄ Troubleshooting\n",
    "\n",
    "| Problem              | Fix                                                       |\n",
    "| -------------------- | --------------------------------------------------------- |\n",
    "| `CUDA out of memory` | Use 4-bit quantization or reduce `max_seq_length`         |\n",
    "| Model loads on CPU   | Verify GPU with `torch.cuda.is_available()`               |\n",
    "| No streaming output  | Ensure you‚Äôre using `TextIteratorStreamer`                |\n",
    "| Template error       | Remove `reasoning_effort` if unsupported in your template |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b98c8",
   "metadata": {},
   "source": [
    "## üìú Credits\n",
    "\n",
    "- Models: Unsloth GPT-OSS family\n",
    "\n",
    "- Libraries: unsloth, transformers, accelerate, bitsandbytes\n",
    "\n",
    "- Author: Adapted for educational and research use üßë‚Äçüíª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c01475",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
